{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentiment analysis**"
      ],
      "metadata": {
        "id": "kpLDApZGgfIy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "457vYKq9gKAb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "# sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/Restaurant_Reviews.tsv', sep = '\\t')\n",
        "data.info()"
      ],
      "metadata": {
        "id": "mHT4xk4vjG25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe9a0f4-c7aa-4b2c-c574-296cb02fa928"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  1000 non-null   object\n",
            " 1   Liked   1000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Exploring Data**"
      ],
      "metadata": {
        "id": "SLXSO-6hj99B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=data['Liked'], palette=['red', 'blue'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "xUaHYoEGjhbR",
        "outputId": "1b3a5691-c370-4470-eb39-9c00a81b1e9a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-108-47d21a0a962c>:1: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(x=data['Liked'], palette=['red', 'blue'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='Liked', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Review letter count'] = data['Review'].apply(len)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "-qRGpV4UkCOw",
        "outputId": "89e070c9-5be5-4545-9f28-73c61d5436f0"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Review  Liked  \\\n",
              "0                             Wow... Loved this place.      1   \n",
              "1                                   Crust is not good.      0   \n",
              "2            Not tasty and the texture was just nasty.      0   \n",
              "3    Stopped by during the late May bank holiday of...      1   \n",
              "4    The selection on the menu was great and so wer...      1   \n",
              "..                                                 ...    ...   \n",
              "995  I think food should have flavor and texture an...      0   \n",
              "996                           Appetite instantly gone.      0   \n",
              "997  Overall I was not impressed and would not go b...      0   \n",
              "998  The whole experience was underwhelming, and I ...      0   \n",
              "999  Then, as if I hadn't wasted enough of my life ...      0   \n",
              "\n",
              "     Review letter count  \n",
              "0                     24  \n",
              "1                     18  \n",
              "2                     41  \n",
              "3                     87  \n",
              "4                     59  \n",
              "..                   ...  \n",
              "995                   66  \n",
              "996                   24  \n",
              "997                   50  \n",
              "998                   91  \n",
              "999                  134  \n",
              "\n",
              "[1000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec48b7b6-aeb3-402f-9a32-ba121b931f92\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "      <th>Review letter count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>I think food should have flavor and texture an...</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Appetite instantly gone.</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Overall I was not impressed and would not go b...</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>The whole experience was underwhelming, and I ...</td>\n",
              "      <td>0</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
              "      <td>0</td>\n",
              "      <td>134</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec48b7b6-aeb3-402f-9a32-ba121b931f92')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec48b7b6-aeb3-402f-9a32-ba121b931f92 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec48b7b6-aeb3-402f-9a32-ba121b931f92');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2876dd7f-d43b-4840-afe1-b3975ab2d27d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2876dd7f-d43b-4840-afe1-b3975ab2d27d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2876dd7f-d43b-4840-afe1-b3975ab2d27d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_75ea6553-c81b-4e69-9512-1e01bfd4e80b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_75ea6553-c81b-4e69-9512-1e01bfd4e80b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 996,\n        \"samples\": [\n          \"They were excellent.\",\n          \"Your servers suck, wait, correction, our server Heimer sucked.\",\n          \"Will be back again!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Liked\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review letter count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32,\n        \"min\": 11,\n        \"max\": 149,\n        \"num_unique_values\": 134,\n        \"samples\": [\n          122,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pre-processing Data**"
      ],
      "metadata": {
        "id": "EpYL_oDhR9Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "vyQRv_mYQ-Yd"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aiSknf0RBj-",
        "outputId": "349d71e5-63be-440b-ca13-19eb6732e4fd"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BECE3m4nSMqp",
        "outputId": "8c781b65-ddce-44e0-bab9-3ff2a290aeca"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = data['Review'][0]\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9cNCdzBHSeVE",
        "outputId": "dcf73173-dd9a-498a-885b-d6a6f31b0991"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Wow... Loved this place.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "s = re.sub('[^a-zA-Z]',\" \",s)\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2FFcE9CAOvr2",
        "outputId": "65df2082-2112-40ae-eb01-42d7cfe120bf"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Wow    Loved this place '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = s.lower()\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oEWD3iFuPhRW",
        "outputId": "4a722824-a8f3-4afc-da44-f2c914732d7f"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wow    loved this place '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = s.split()\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fmnO4TJQKyd",
        "outputId": "ee9f72cf-5347-413e-84a3-2367484dd046"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wow', 'loved', 'this', 'place']"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = []\n",
        "\n",
        "for word in s :\n",
        "  if word not in stopwords.words('english'):\n",
        "    temp.append(word)\n",
        "\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7E0J1GuS7bF",
        "outputId": "ce46327d-fe00-4597-a0ee-86ff9f3541d4"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wow', 'loved', 'place']"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[word for word in s if word not in stopwords.words('english')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnivzDi-xni6",
        "outputId": "eeb69ad6-7440-443f-d23c-fce181cb6543"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wow', 'loved', 'place']"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = ' '.join(temp)\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kjFbNVZbslEp",
        "outputId": "766ea4b9-7629-4ef4-e1ef-1ef157b4caf3"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wow loved place'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "KZzJ3QOhsy17"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "K3_dOHY3s6LR"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = ps.stem(s)\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NnVUI_0Hs9OR",
        "outputId": "c9dee125-d16a-44bb-ccc5-bcde0c87b27c"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wow loved plac'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "eyoxRAjftZWt"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()\n",
        "cv.fit_transform(s.split()).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8KKUUbSthTz",
        "outputId": "55148177-3c7c-4ba2-9821-e29a1a11b2f5"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "\n",
        "for i in range (len(data)):\n",
        "  s = re.sub('[^a-zA-Z]',\" \",data['Review'][i])\n",
        "  s = s.lower()\n",
        "  s = s.split()\n",
        "  s = [word for word in s if word not in stopwords.words('english')]\n",
        "  s = ' '.join(s)\n",
        "  s = ps.stem(s)\n",
        "  corpus.append(s)\n",
        "\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3Skw6BrxiuR",
        "outputId": "c3b2238d-ea12-4a31-9861-27aa34ebf034"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wow loved plac',\n",
              " 'crust good',\n",
              " 'tasty texture nasti',\n",
              " 'stopped late may bank holiday rick steve recommendation lov',\n",
              " 'selection menu great pric',\n",
              " 'getting angry want damn pho',\n",
              " 'honeslty taste fresh',\n",
              " 'potatoes like rubber could tell made ahead time kept warm',\n",
              " 'fries great',\n",
              " 'great touch',\n",
              " 'service prompt',\n",
              " 'would go back',\n",
              " 'cashier care ever say still ended wayyy overpr',\n",
              " 'tried cape cod ravoli chicken cranberry mmmm',\n",
              " 'disgusted pretty sure human hair',\n",
              " 'shocked signs indicate cash',\n",
              " 'highly recommend',\n",
              " 'waitress little slow servic',\n",
              " 'place worth time let alone vega',\n",
              " 'like',\n",
              " 'burrittos blah',\n",
              " 'food amaz',\n",
              " 'service also cut',\n",
              " 'could care less interior beauti',\n",
              " 'perform',\n",
              " 'right red velvet cake ohhh stuff good',\n",
              " 'never brought salad ask',\n",
              " 'hole wall great mexican street tacos friendly staff',\n",
              " 'took hour get food tables restaurant food luke warm sever running around like totally overwhelm',\n",
              " 'worst salmon sashimi',\n",
              " 'also combos like burger fries beer decent d',\n",
              " 'like final blow',\n",
              " 'found place accident could happi',\n",
              " 'seems like good quick place grab bite familiar pub food favor look elsewher',\n",
              " 'overall like place lot',\n",
              " 'redeeming quality restaurant inexpens',\n",
              " 'ample portions good pric',\n",
              " 'poor service waiter made feel like stupid every time came t',\n",
              " 'first visit hiro delight',\n",
              " 'service suck',\n",
              " 'shrimp tender moist',\n",
              " 'deal good enough would drag establish',\n",
              " 'hard judge whether sides good grossed melted styrofoam want eat fear getting sick',\n",
              " 'positive note server attentive provided great servic',\n",
              " 'frozen pucks disgust worst people behind regist',\n",
              " 'thing like prime rib dessert sect',\n",
              " 'bad food damn gener',\n",
              " 'burger good beef cooked right',\n",
              " 'want sandwich go firehous',\n",
              " 'side greek salad greek dressing tasty pita hummus refresh',\n",
              " 'ordered duck rare pink tender inside nice char outsid',\n",
              " 'came running us realized husband left sunglasses t',\n",
              " 'chow mein good',\n",
              " 'horrible attitudes towards customers talk one customers enjoy food',\n",
              " 'portion hug',\n",
              " 'loved friendly servers great food wonderful imaginative menu',\n",
              " 'heart attack grill downtown vegas absolutely flat lined excuse restaur',\n",
              " 'much seafood like strings pasta bottom',\n",
              " 'salad right amount sauce power scallop perfectly cook',\n",
              " 'ripped banana ripped petrified tasteless',\n",
              " 'least think refill water struggle wave minut',\n",
              " 'place receives stars appet',\n",
              " 'cocktails handmade delici',\n",
              " 'definitely go back',\n",
              " 'glad found plac',\n",
              " 'great food service huge portions give military discount',\n",
              " 'always great time dos gringo',\n",
              " 'update went back second time still amaz',\n",
              " 'got food apparently never heard salt batter fish chewi',\n",
              " 'great way finish great',\n",
              " 'deal included tastings drinks jeff went beyond expect',\n",
              " 'really really good rice tim',\n",
              " 'service meh',\n",
              " 'took min get milkshake nothing chocolate milk',\n",
              " 'guess known place would suck inside excalibur use common sens',\n",
              " 'scallop dish quite appalling value wel',\n",
              " 'times bad customer servic',\n",
              " 'sweet potato fries good seasoned wel',\n",
              " 'today second time lunch buffet pretty good',\n",
              " 'much good food vegas feel cheated wasting eating opportunity going rice compani',\n",
              " 'coming like experiencing underwhelming relationship parties wait person ask break',\n",
              " 'walked place smelled like old grease trap others ',\n",
              " 'turkey roast beef bland',\n",
              " 'place',\n",
              " 'pan cakes everyone raving taste like sugary disaster tailored palate six year old',\n",
              " 'love pho spring rolls oh yummy tri',\n",
              " 'poor batter meat ratio made chicken tenders unsatisfi',\n",
              " 'say food amaz',\n",
              " 'omelets di',\n",
              " 'everything fresh delici',\n",
              " 'summary largely disappointing dining experi',\n",
              " 'like really sexy party mouth outrageously flirting hottest person parti',\n",
              " 'never hard rock casino never ever step forward',\n",
              " 'best breakfast buffet',\n",
              " 'say bye bye tip ladi',\n",
              " 'never go',\n",
              " 'back',\n",
              " 'food arrived quickli',\n",
              " 'good',\n",
              " 'side cafe serves really good food',\n",
              " 'server fantastic found wife loves roasted garlic bone marrow added extra meal another marrow go',\n",
              " 'good thing waiter helpful kept bloddy mary com',\n",
              " 'best buffet town price cannot beat',\n",
              " 'loved mussels cooked wine reduction duck tender potato dishes delici',\n",
              " 'one better buffet',\n",
              " 'went tigerlilly fantastic afternoon',\n",
              " 'food delicious bartender attentive personable got great d',\n",
              " 'ambience wonderful music play',\n",
              " 'go back next trip',\n",
              " 'sooooo good',\n",
              " 'real sushi lovers let honest yama good',\n",
              " 'least min passed us ordering food arriving busi',\n",
              " 'really fantastic thai restaurant definitely worth visit',\n",
              " 'nice spicy tend',\n",
              " 'good pric',\n",
              " 'check',\n",
              " 'pretty gross',\n",
              " 'better atmospher',\n",
              " 'kind hard mess steak',\n",
              " 'although much liked look sound place actual experience bit disappoint',\n",
              " 'know place managed served blandest food ever eaten preparing indian cuisin',\n",
              " 'worst service boot least worri',\n",
              " 'service fine waitress friendli',\n",
              " 'guys steaks steak loving son steak best worst places said best steak ever eaten',\n",
              " 'thought venture away get good sushi place really hit spot night',\n",
              " 'host staff lack better word bitch',\n",
              " 'bland liking place number reasons want waste time bad reviewing leav',\n",
              " 'phenomenal food service ambi',\n",
              " 'return',\n",
              " 'definitely worth venturing strip pork belly return next time vega',\n",
              " 'place way overpriced mediocre food',\n",
              " 'penne vodka excel',\n",
              " 'good selection food including massive meatloaf sandwich crispy chicken wrap delish tuna melt tasty burg',\n",
              " 'management rud',\n",
              " 'delicious nyc bagels good selections cream cheese real lox capers even',\n",
              " 'great subway fact good come every subway meet expect',\n",
              " 'seriously solid breakfast',\n",
              " 'one best bars food vega',\n",
              " 'extremely rude really many restaurants would love dine weekend vega',\n",
              " 'drink never empty made really great menu suggest',\n",
              " '',\n",
              " 'waiter helpful friendly rarely checked u',\n",
              " 'husband ate lunch disappointed food servic',\n",
              " 'red curry much bamboo shoots tasti',\n",
              " 'nice blanket moz top feel like done cover subpar food',\n",
              " 'bathrooms clean place well decor',\n",
              " 'menu always changing food quality going service extremely slow',\n",
              " 'service little slow considering served people servers food coming slow pac',\n",
              " 'give thumb',\n",
              " 'watched waiter pay lot attention tables ignore u',\n",
              " 'fianc came middle day greeted seated right away',\n",
              " 'great restaurant mandalay bay',\n",
              " 'waited forty five minutes vain',\n",
              " 'crostini came salad stal',\n",
              " 'highlights great quality nigiri',\n",
              " 'staff friendly joint always clean',\n",
              " 'different cut piece day still wonderful tender well well flavor',\n",
              " 'ordered voodoo pasta first time really excellent pasta since going gluten free several years ago',\n",
              " 'place good',\n",
              " 'unfortunately must hit bakery leftover day everything ordered stal',\n",
              " 'came back today since relocated still impress',\n",
              " 'seated immedi',\n",
              " 'menu diverse reasonably pr',\n",
              " 'avoid cost',\n",
              " 'restaurant always full never wait',\n",
              " 'delici',\n",
              " 'place hands one best places eat phoenix metro area',\n",
              " 'go looking good food',\n",
              " 'never treated bad',\n",
              " 'bacon hella salti',\n",
              " 'also ordered spinach avocado salad ingredients sad dressing literally zero tast',\n",
              " 'really vegas fine dining used right menus handed ladies prices list',\n",
              " 'waitresses friendli',\n",
              " 'lordy khao soi dish missed curry lov',\n",
              " 'everything menu terrific also thrilled made amazing accommodations vegetarian daught',\n",
              " 'perhaps caught night judging reviews inspired go back',\n",
              " 'service leaves lot desir',\n",
              " 'atmosphere modern hip maintaining touch cozi',\n",
              " 'weekly haunt definitely place come back everi',\n",
              " 'literally sat minutes one asking take ord',\n",
              " 'burger absolutely flavor meat totally bland burger overcooked charcoal flavor',\n",
              " 'also decided send back waitress looked like verge heart attack',\n",
              " 'dressed treated rud',\n",
              " 'probably dirt',\n",
              " 'love place hits spot want something healthy lacking quantity flavor',\n",
              " 'ordered lemon raspberry ice cocktail also incred',\n",
              " 'food sucked expected sucked could imagin',\n",
              " 'interesting decor',\n",
              " 'really like crepe st',\n",
              " 'also served hot bread butter home made potato chips bacon bits top original good',\n",
              " 'watch preparing delicious food',\n",
              " 'egg rolls fantast',\n",
              " 'order arrived one gyros miss',\n",
              " 'salad wings ice cream dessert left feeling quite satisfi',\n",
              " 'really sure joey voted best hot dog valley readers phoenix magazin',\n",
              " 'best place go tasty bowl pho',\n",
              " 'live music fridays totally blow',\n",
              " 'never insulted felt disrespect',\n",
              " 'friendly staff',\n",
              " 'worth driv',\n",
              " 'heard good things place exceeding every hope could dream',\n",
              " 'food great serivc',\n",
              " 'warm beer help',\n",
              " 'great brunch spot',\n",
              " 'service friendly invit',\n",
              " 'good lunch spot',\n",
              " 'lived since first last time stepped foot plac',\n",
              " 'worst experience ev',\n",
              " 'must night plac',\n",
              " 'sides delish mixed mushrooms yukon gold puree white corn beat',\n",
              " 'bug never showed would given sure side wall bug climbing kitchen',\n",
              " 'minutes waiting salad realized coming time soon',\n",
              " 'friend loved salmon tartar',\n",
              " 'go back',\n",
              " 'extremely tasti',\n",
              " 'waitress good though',\n",
              " 'soggy good',\n",
              " 'jamaican mojitos delici',\n",
              " 'small worth pric',\n",
              " 'food rich order accordingli',\n",
              " 'shower area outside rinse take full shower unless mind nude everyone se',\n",
              " 'service bit lack',\n",
              " 'lobster bisque bussell sprouts risotto filet needed salt pepper course none t',\n",
              " 'hopefully bodes going business someone cook com',\n",
              " 'either cold enough flavor bad',\n",
              " 'loved bacon wrapped d',\n",
              " 'unbelievable bargain',\n",
              " 'folks otto always make us feel welcome speci',\n",
              " 'mains also uninspir',\n",
              " 'place first pho amaz',\n",
              " 'wonderful experience made place must stop whenever town',\n",
              " 'food bad enough enjoy dealing world worst annoying drunk peopl',\n",
              " 'fun chef',\n",
              " 'ordered double cheeseburger got single patty falling apart picture uploaded yeah still suck',\n",
              " 'great place couple drinks watch sporting events walls covered tv',\n",
              " 'possible give zero star',\n",
              " 'descriptions said yum yum sauce another said eel sauce yet another said spicy mayo well none rolls sauc',\n",
              " 'say would hardest decision honestly dishes taste supposed taste amaz',\n",
              " 'rolled eyes may stayed sure go back tri',\n",
              " 'everyone attentive providing excellent customer servic',\n",
              " 'horrible waste time money',\n",
              " 'dish quite flavour',\n",
              " 'time side restaurant almost empty excus',\n",
              " 'busy either also building freezing cold',\n",
              " 'like reviewer said pay eat plac',\n",
              " 'drinks took close minutes come one point',\n",
              " 'seriously flavorful delights folk',\n",
              " 'much better ayce sushi place went vega',\n",
              " 'lighting dark enough set mood',\n",
              " 'based sub par service received effort show gratitude business going back',\n",
              " 'owner really great peopl',\n",
              " 'nothing privileged working ',\n",
              " 'greek dressing creamy flavor',\n",
              " 'overall think would take parents place made similar complaints silently felt',\n",
              " 'pizza good peanut sauce tasti',\n",
              " 'table service pretty fast',\n",
              " 'fantastic servic',\n",
              " 'well would given godfathers zero stars poss',\n",
              " 'know mak',\n",
              " 'tough short flavor',\n",
              " 'hope place sticks around',\n",
              " 'bars vegas ever recall charged tap wat',\n",
              " 'restaurant atmosphere exquisit',\n",
              " 'good service clean inexpensive boot',\n",
              " 'seafood fresh generous port',\n",
              " 'plus buck',\n",
              " 'service par eith',\n",
              " 'thus far visited twice food absolutely delicious tim',\n",
              " 'good year ago',\n",
              " 'self proclaimed coffee cafe wildly disappoint',\n",
              " 'veggitarian platter world',\n",
              " 'cant go wrong food',\n",
              " 'beat',\n",
              " 'stopped place madison ironman friendly kind staff',\n",
              " 'chefs friendly good job',\n",
              " 'better dedicated boba tea spots even jenni pho',\n",
              " 'liked patio service outstand',\n",
              " 'goat taco skimp meat wow flavor',\n",
              " 'think',\n",
              " 'mac salad pretty bland get',\n",
              " 'went bachi burger friend recommendation disappoint',\n",
              " 'service stink',\n",
              " 'waited wait',\n",
              " 'place quality sushi quality restaur',\n",
              " 'would definitely recommend wings well pizza',\n",
              " 'great pizza salad',\n",
              " 'things went wrong burned saganaki',\n",
              " 'waited hour breakfast could done times better hom',\n",
              " 'place amaz',\n",
              " 'hate disagree fellow yelpers husband disappointed plac',\n",
              " 'waited hours never got either pizzas many around us came lat',\n",
              " 'know slow',\n",
              " 'staff great food delish incredible beer select',\n",
              " 'live neighborhood disappointed back convenient loc',\n",
              " 'know pulled pork could soooo delici',\n",
              " 'get incredibly fresh fish prepared car',\n",
              " 'go gave star rating please know third time eating bachi burger writing review',\n",
              " 'love fact everything menu worth',\n",
              " 'never dining plac',\n",
              " 'food excellent service good',\n",
              " 'good beer drink selection good food select',\n",
              " 'please stay away shrimp stir fried noodl',\n",
              " 'potato chip order sad could probably count many chips box probably around',\n",
              " 'food really bor',\n",
              " 'good service check',\n",
              " 'greedy corporation never see another dim',\n",
              " 'never ever go back',\n",
              " 'much like go back get passed atrocious service never return',\n",
              " 'summer dine charming outdoor patio delight',\n",
              " 'expect good',\n",
              " 'fantastic food',\n",
              " 'ordered toasted english muffin came untoast',\n",
              " 'food good',\n",
              " 'never going back',\n",
              " 'great food price high quality house mad',\n",
              " 'bus boy hand rud',\n",
              " 'point friends basically figured place joke mind making publicly loudly known',\n",
              " 'back good bbq lighter fare reasonable pricing tell public back old way',\n",
              " 'considering two us left full happy go wrong',\n",
              " 'bread made hous',\n",
              " 'downside servic',\n",
              " 'also fries without doubt worst fries ev',\n",
              " 'service exceptional food good review',\n",
              " 'couple months later returned amazing m',\n",
              " 'favorite place town shawarrrrrrma',\n",
              " 'black eyed peas sweet potatoes unr',\n",
              " 'disappoint',\n",
              " 'could serve vinaigrette may make better overall dish still good',\n",
              " 'go far many places never seen restaurant serves egg breakfast especi',\n",
              " 'mom got home immediately got sick bites salad',\n",
              " 'servers pleasant deal always honor pizza hut coupon',\n",
              " 'truly unbelievably good glad went back',\n",
              " 'fantastic service pleased atmospher',\n",
              " 'everything gross',\n",
              " 'love plac',\n",
              " 'great service food',\n",
              " 'first bathrooms location dirty seat covers replenished plain yucki',\n",
              " 'burger got gold standard burger kind disappoint',\n",
              " 'omg food delicioso',\n",
              " 'nothing authentic plac',\n",
              " 'spaghetti nothing special whatsoev',\n",
              " 'dishes salmon best great',\n",
              " 'vegetables fresh sauce feels like authentic thai',\n",
              " 'worth driving tucson',\n",
              " 'selection probably worst seen vegas non',\n",
              " 'pretty good beer select',\n",
              " 'place like chipotle bett',\n",
              " 'classy warm atmosphere fun fresh appetizers succulent steaks baseball steak',\n",
              " 'stars brick oven bread app',\n",
              " 'eaten multiple times time food delici',\n",
              " 'sat another ten minutes finally gave left',\n",
              " 'terribl',\n",
              " 'everyone treated equally speci',\n",
              " 'take min pancakes egg',\n",
              " 'delici',\n",
              " 'good side staff genuinely pleasant enthusiastic real treat',\n",
              " 'sadly gordon ramsey steak place shall sharply avoid next trip vega',\n",
              " 'always evening wonderful food delici',\n",
              " 'best fish ever lif',\n",
              " 'bathroom next door nic',\n",
              " 'buffet small food offered bland',\n",
              " 'outstanding little restaurant best food ever tast',\n",
              " 'pretty cool would say',\n",
              " 'definitely turn doubt back unless someone else buy',\n",
              " 'server great job handling large rowdy t',\n",
              " 'find wasting food despicable food',\n",
              " 'wife lobster bisque soup lukewarm',\n",
              " 'would come back sushi craving vega',\n",
              " 'staff great ambiance great',\n",
              " 'deserves star',\n",
              " 'left stomach ache felt sick rest day',\n",
              " 'dropped bal',\n",
              " 'dining space tiny elegantly decorated comfort',\n",
              " 'customize order way like usual eggplant green bean stir fry lov',\n",
              " 'beans rice mediocre best',\n",
              " 'best tacos town far',\n",
              " 'took back money got outta',\n",
              " 'interesting part town place amaz',\n",
              " 'rude inconsiderate manag',\n",
              " 'staff friendly wait times served horrible one even says hi first minut',\n",
              " 'back',\n",
              " 'great dinn',\n",
              " 'service outshining definitely recommend halibut',\n",
              " 'food terr',\n",
              " 'never ever go back told many people happen',\n",
              " 'recommend unless car breaks front starv',\n",
              " 'come back every time vega',\n",
              " 'place deserves one star food',\n",
              " 'disgrac',\n",
              " 'def coming back bowl next tim',\n",
              " 'want healthy authentic ethic food try plac',\n",
              " 'continue come ladies night andddd date night highly recommend place anyone area',\n",
              " 'several times past experience always great',\n",
              " 'walked away stuffed happy first vegas buffet experi',\n",
              " 'service excellent prices pretty reasonable considering vegas located inside crystals shopping mall aria',\n",
              " 'summarize food incredible nay transcendant nothing brings joy quite like memory pneumatic condiment dispens',\n",
              " 'probably one people ever go ians lik',\n",
              " 'kids pizza always hit lots great side dish options kiddo',\n",
              " 'service perfect family atmosphere nice se',\n",
              " 'cooked perfection service impecc',\n",
              " 'one simply disappoint',\n",
              " 'overall disappointed quality food bouchon',\n",
              " 'accountant know getting screw',\n",
              " 'great place eat reminds little mom pop shops san francisco bay area',\n",
              " 'today first taste buldogis gourmet hot dog tell ever thought poss',\n",
              " 'left frustr',\n",
              " 'definitely soon',\n",
              " 'food really good got full petty fast',\n",
              " 'service fantast',\n",
              " 'total waste tim',\n",
              " 'know kind best iced tea',\n",
              " 'come hungry leave happy stuf',\n",
              " 'service give star',\n",
              " 'assure disappoint',\n",
              " 'take little bad service food suck',\n",
              " 'gave trying eat crust teeth still sor',\n",
              " 'completely gross',\n",
              " 'really enjoyed ',\n",
              " 'first time going think quickly become regular',\n",
              " 'server nice even though looked little overwhelmed needs stayed professional friendly end',\n",
              " 'dinner companions told everything fresh nice texture tast',\n",
              " 'ground right next table large smeared stepped tracked everywhere pile green bird poop',\n",
              " 'furthermore even find hours operation websit',\n",
              " 'tried like place times think don',\n",
              " 'mistak',\n",
              " 'complaint',\n",
              " 'seriously good pizza expert connisseur top',\n",
              " 'waiter jerk',\n",
              " 'strike wants rush',\n",
              " 'nicest restaurant owners ever come across',\n",
              " 'never com',\n",
              " 'loved biscuit',\n",
              " 'service quick friendli',\n",
              " 'ordered appetizer took minutes pizza another minut',\n",
              " 'absolutley fantast',\n",
              " 'huge awkward lb piece cow ths gristle fat',\n",
              " 'definitely come back',\n",
              " 'like steiners dark feels like bar',\n",
              " 'wow spicy delici',\n",
              " 'familiar check',\n",
              " 'take business dinner dollars elsewher',\n",
              " 'love go back',\n",
              " 'anyway fs restaurant wonderful breakfast lunch',\n",
              " 'nothing speci',\n",
              " 'day week different deal delici',\n",
              " 'mention combination pears almonds bacon big winn',\n",
              " 'back',\n",
              " 'sauce tasteless',\n",
              " 'food delicious spicy enough sure ask spicier prefer way',\n",
              " 'ribeye steak cooked perfectly great mesquite flavor',\n",
              " 'think going back anytime soon',\n",
              " 'food gooodd',\n",
              " 'far sushi connoisseur definitely tell difference good food bad food certainly bad food',\n",
              " 'insult',\n",
              " 'last times lunch bad',\n",
              " 'chicken wings contained driest chicken meat ever eaten',\n",
              " 'food good enjoyed every mouthful enjoyable relaxed venue couples small family groups etc',\n",
              " 'nargile think great',\n",
              " 'best tater tots southwest',\n",
              " 'loved plac',\n",
              " 'definitely worth paid',\n",
              " 'vanilla ice cream creamy smooth profiterole choux pastry fresh enough',\n",
              " 'im az time new spot',\n",
              " 'manager worst',\n",
              " 'inside really quite nice clean',\n",
              " 'food outstanding prices reason',\n",
              " 'think running back carly anytime soon food',\n",
              " 'due fact took minutes acknowledged another minutes get food kept forgetting th',\n",
              " 'love margarita',\n",
              " 'first vegas buffet disappoint',\n",
              " 'good though',\n",
              " 'one note ventilation could use upgrad',\n",
              " 'great pork sandwich',\n",
              " 'waste tim',\n",
              " 'total letdown would much rather go camelback flower shop cartel coffe',\n",
              " 'third cheese friend burger cold',\n",
              " 'enjoy pizza brunch',\n",
              " 'steaks well trimmed also perfectly cook',\n",
              " 'group claimed would handled us beauti',\n",
              " 'love',\n",
              " 'asked bill leave without eating bring eith',\n",
              " 'place jewel las vegas exactly hoping find nearly ten years liv',\n",
              " 'seafood limited boiled shrimp crab legs crab legs definitely taste fresh',\n",
              " 'selection food best',\n",
              " 'delicious absolutely back',\n",
              " 'small family restaurant fine dining establish',\n",
              " 'toro tartare cavier extraordinary liked thinly sliced wagyu white truffl',\n",
              " 'dont think back long tim',\n",
              " 'attached gas station rarely good sign',\n",
              " 'awesom',\n",
              " 'back many times soon',\n",
              " 'menu much good stuff could decid',\n",
              " 'worse humiliated worker right front bunch horrible name cal',\n",
              " 'conclusion filling m',\n",
              " 'daily specials always hit group',\n",
              " 'tragedy struck',\n",
              " 'pancake also really good pretty larg',\n",
              " 'first crawfish experience delici',\n",
              " 'monster chicken fried steak eggs time favorit',\n",
              " 'waitress sweet funni',\n",
              " 'also taste mom multi grain pumpkin pancakes pecan butter amazing fluffy delici',\n",
              " 'rather eat airline food seri',\n",
              " 'cant say enough good things plac',\n",
              " 'ambiance incred',\n",
              " 'waitress manager friendli',\n",
              " 'would recommend plac',\n",
              " 'overall impressed noca',\n",
              " 'gyro basically lettuc',\n",
              " 'terrible servic',\n",
              " 'thoroughly disappoint',\n",
              " 'much pasta love homemade hand made pastas thin pizza',\n",
              " 'give try happi',\n",
              " 'far best cheesecurds ev',\n",
              " 'reasonably priced also',\n",
              " 'everything perfect night',\n",
              " 'food good typical bar food',\n",
              " 'drive get',\n",
              " 'first glance lovely bakery cafe nice ambiance clean friendly staff',\n",
              " 'anyway think go back',\n",
              " 'point finger item menu order disappoint',\n",
              " 'oh thing beauty restaur',\n",
              " 'gone go',\n",
              " 'greasy unhealthy m',\n",
              " 'first time might last',\n",
              " 'burgers amaz',\n",
              " 'similarly delivery man say word apology food minutes l',\n",
              " 'way expens',\n",
              " 'sure order dessert even need pack go tiramisu cannoli di',\n",
              " 'first time wait next',\n",
              " 'bartender also nic',\n",
              " 'everything good tasti',\n",
              " 'place two thumbs way',\n",
              " 'best place vegas breakfast check sat sun',\n",
              " 'love authentic mexican food want whole bunch interesting yet delicious meats choose need try plac',\n",
              " 'terrible manag',\n",
              " 'excellent new restaurant experienced frenchman',\n",
              " 'zero stars would give zero star',\n",
              " 'great steak great sides great wine amazing dessert',\n",
              " 'worst martini ev',\n",
              " 'steak shrimp opinion best entrees gc',\n",
              " 'opportunity today sample amazing pizza',\n",
              " 'waited thirty minutes seated although vacant tables folks wait',\n",
              " 'yellowtail carpaccio melt mouth fresh',\n",
              " 'try going back even empti',\n",
              " 'going eat potato found strangers hair',\n",
              " 'spicy enough perfect actu',\n",
              " 'last night second time dining happy decided go back',\n",
              " 'even hello right',\n",
              " 'desserts bit strang',\n",
              " 'boyfriend came first time recent trip vegas could pleased quality food servic',\n",
              " 'really recommend place go wrong donut plac',\n",
              " 'nice ambi',\n",
              " 'would recommend saving room',\n",
              " 'guess maybe went night disgrac',\n",
              " 'however recent experience particular location good',\n",
              " 'know like restaurants someth',\n",
              " 'avoid establish',\n",
              " 'think restaurant suffers trying hard enough',\n",
              " 'tapas dishes delici',\n",
              " 'heart plac',\n",
              " 'salad bland vinegrette baby greens hearts palm',\n",
              " 'two felt disgust',\n",
              " 'good tim',\n",
              " 'believe place great stop huge belly hankering sushi',\n",
              " 'generous portions great tast',\n",
              " 'never go back place never ever recommended place anyon',\n",
              " 'servers went back forth several times even much help',\n",
              " 'food delici',\n",
              " 'hour seri',\n",
              " 'consider theft',\n",
              " 'eew location needs complete overhaul',\n",
              " 'recently witnessed poor quality management towards guests wel',\n",
              " 'waited waited wait',\n",
              " 'also came back check us regularly excellent servic',\n",
              " 'server super nice checked us many tim',\n",
              " 'pizza tasted old super chewy good way',\n",
              " 'swung give try deeply disappoint',\n",
              " 'service good company bett',\n",
              " 'staff also friendly effici',\n",
              " 'service fan quick served nice folk',\n",
              " 'boy sucker dri',\n",
              " 'rate',\n",
              " 'look authentic thai food go els',\n",
              " 'steaks recommend',\n",
              " 'pulled car waited another minutes acknowledg',\n",
              " 'great food great service clean friendly set',\n",
              " 'assure back',\n",
              " 'hate things much cheap quality black ol',\n",
              " 'breakfast perpared great beautiful presentation giant slices toast lightly dusted powdered sugar',\n",
              " 'kids play area nasti',\n",
              " 'great place fo take eat',\n",
              " 'waitress friendly happy accomodate vegan veggie opt',\n",
              " 'omg felt like never eaten thai food dish',\n",
              " 'extremely crumby pretty tasteless',\n",
              " 'pale color instead nice char flavor',\n",
              " 'croutons also taste homemade extra plu',\n",
              " 'got home see driest damn wings ev',\n",
              " 'regular stop trips phoenix',\n",
              " 'really enjoyed crema caf expanded even told friends best breakfast',\n",
              " 'good money',\n",
              " 'miss wish one philadelphia',\n",
              " 'got sitting fairly fast ended waiting minutes place order another minutes food arriv',\n",
              " 'also best cheese crisp town',\n",
              " 'good value great food great servic',\n",
              " 'ask satisfying m',\n",
              " 'food good',\n",
              " 'awesom',\n",
              " 'wanted leav',\n",
              " 'made drive way north scottsdale one bit disappoint',\n",
              " 'eat',\n",
              " 'owners really really need quit soooooo cheap let wrap freaking sandwich two papers on',\n",
              " 'checked place couple years ago impress',\n",
              " 'chicken got definitely reheated ok wedges cold soggi',\n",
              " 'sorry getting food anytime soon',\n",
              " 'absolute must visit',\n",
              " 'cow tongue cheek tacos amaz',\n",
              " 'friend like bloody mari',\n",
              " 'despite hard rate businesses actually rare give star',\n",
              " 'really want make experience good on',\n",
              " 'return',\n",
              " 'chicken pho tasted bland',\n",
              " 'disappoint',\n",
              " 'grilled chicken tender yellow saffron season',\n",
              " 'drive thru means want wait around half hour food somehow end going make us wait wait',\n",
              " 'pretty awesome plac',\n",
              " 'ambience perfect',\n",
              " 'best luck rude non customer service focused new manag',\n",
              " 'grandmother make roasted chicken better on',\n",
              " 'asked multiple times wine list time ignored went hostess got on',\n",
              " 'staff always super friendly helpful especially cool bring two small boys babi',\n",
              " 'four stars food guy blue shirt great vibe still letting us eat',\n",
              " 'roast beef sandwich tasted really good',\n",
              " 'evening drastically sick',\n",
              " 'high quality chicken chicken caesar salad',\n",
              " 'ordered burger rare came don',\n",
              " 'promptly greeted s',\n",
              " 'tried go lunch madhous',\n",
              " 'proven dead wrong sushi bar quality great service fast food impecc',\n",
              " 'waiting hour seated greatest mood',\n",
              " 'good joint',\n",
              " 'macarons insanely good',\n",
              " 'eat',\n",
              " 'waiter attentive friendly inform',\n",
              " 'maybe cold would somewhat ed',\n",
              " 'place lot promise fails deliv',\n",
              " 'bad experi',\n",
              " 'mistak',\n",
              " 'food average best',\n",
              " 'great food',\n",
              " 'going back anytime soon',\n",
              " 'disappointed ordered big bay plat',\n",
              " 'great place relax awesome burger b',\n",
              " 'perfect sit family meal get together friend',\n",
              " 'much flavor poorly construct',\n",
              " 'patio seating comfort',\n",
              " 'fried rice dry wel',\n",
              " 'hands favorite italian restaur',\n",
              " 'screams legit book somethat also pretty rare vega',\n",
              " 'fun experi',\n",
              " 'atmosphere great lovely duo violinists playing songs request',\n",
              " 'personally love hummus pita baklava falafels baba ganoush amazing eggpl',\n",
              " 'convenient since staying mgm',\n",
              " 'owners super friendly staff court',\n",
              " 'great',\n",
              " 'eclectic select',\n",
              " 'sweet potato tots good onion rings perfection clos',\n",
              " 'staff attent',\n",
              " 'chef generous time even came around twice take pictur',\n",
              " 'owner used work nobu place really similar half pric',\n",
              " 'google mediocre imagine smashburger pop',\n",
              " 'dont go',\n",
              " 'promise disappoint',\n",
              " 'sushi lover avoid place mean',\n",
              " 'great double cheeseburg',\n",
              " 'awesome service food',\n",
              " 'fantastic neighborhood gem',\n",
              " 'wait go back',\n",
              " 'plantains worst ever tast',\n",
              " 'great place highly recommend',\n",
              " 'service slow attent',\n",
              " 'gave stars giving star',\n",
              " 'staff spends time talk',\n",
              " 'dessert panna cotta amaz',\n",
              " 'good food great atmospher',\n",
              " 'damn good steak',\n",
              " 'total brunch fail',\n",
              " 'prices reasonable flavors spot sauce home made slaw drenched mayo',\n",
              " 'decor nice piano music soundtrack pleas',\n",
              " 'steak amazing rge fillet relleno best seafood plate ev',\n",
              " 'good food good servic',\n",
              " 'absolutely amaz',\n",
              " 'probably back honest',\n",
              " 'definitely back',\n",
              " 'sergeant pepper beef sandwich auju sauce excellent sandwich wel',\n",
              " 'hawaiian breeze mango magic pineapple delight smoothies tried far good',\n",
              " 'went lunch service slow',\n",
              " 'much say place walked expected amazing quickly disappoint',\n",
              " 'mortifi',\n",
              " 'needless say never back',\n",
              " 'anyways food definitely filling price pay expect',\n",
              " 'chips came dripping grease mostly ed',\n",
              " 'really impressed strip steak',\n",
              " 'going since every meal awesom',\n",
              " 'server nice attentive serving staff',\n",
              " 'cashier friendly even brought food',\n",
              " 'work hospitality industry paradise valley refrained recommending cibo long',\n",
              " 'atmosphere fun',\n",
              " 'would recommend oth',\n",
              " 'service quick even go orders like lik',\n",
              " 'mean really get famous fish chips terr',\n",
              " 'said mouths bellies still quite pleas',\n",
              " 'thing',\n",
              " 'thumb',\n",
              " 'reading please go',\n",
              " 'loved grilled pizza reminded legit italian pizza',\n",
              " 'pros large seating area nice bar area great simple drink menu best brick oven pizza homemade dough',\n",
              " 'really nice atmospher',\n",
              " 'tonight elk filet special suck',\n",
              " 'one bite hook',\n",
              " 'ordered old classics new dishes going times sorely disappointed everyth',\n",
              " 'cute quaint simple honest',\n",
              " 'chicken deliciously seasoned perfect fry outside moist chicken insid',\n",
              " 'food great always compliments chef',\n",
              " 'special thanks dylan recommendation order yummy tummi',\n",
              " 'awesome selection b',\n",
              " 'great food awesome servic',\n",
              " 'one nice thing added gratuity bill since party larger expect tip',\n",
              " 'fly apple juice fli',\n",
              " 'han nan chicken also tasti',\n",
              " 'service thought good',\n",
              " 'food barely lukewarm must sitting waiting server bring u',\n",
              " 'ryan bar definitely one edinburgh establishment revisit',\n",
              " 'nicest chinese restaur',\n",
              " 'overall like food servic',\n",
              " 'also serve indian naan bread hummus spicy pine nut sauce world',\n",
              " 'probably never coming back recommend',\n",
              " 'friend pasta also bad barely touch',\n",
              " 'try airport experience tasty food speedy friendly servic',\n",
              " 'love decor chinese calligraphy wall pap',\n",
              " 'never anything complain',\n",
              " 'restaurant clean family restaurant feel',\n",
              " 'way fri',\n",
              " 'sure long stood long enough begin feel awkwardly plac',\n",
              " 'opened sandwich impressed good way',\n",
              " 'back',\n",
              " 'warm feeling service felt like guest special treat',\n",
              " 'extensive menu provides lots options breakfast',\n",
              " 'always order vegetarian menu dinner wide array options choos',\n",
              " 'watched prices inflate portions get smaller management attitudes grow rapidli',\n",
              " 'wonderful lil tapas ambience made feel warm fuzzy insid',\n",
              " 'got enjoy seafood salad fabulous vinegrett',\n",
              " 'wontons thin thick chewy almost melt mouth',\n",
              " 'level spicy perfect spice whelm soup',\n",
              " 'sat right time server get go fantast',\n",
              " 'main thing enjoy crowd older crowd around mid',\n",
              " 'side town definitely spot hit',\n",
              " 'wait minutes get drink longer get arepa',\n",
              " 'great place eat',\n",
              " 'jalapeno bacon soooo good',\n",
              " 'service poor thats nic',\n",
              " 'food good service good prices good',\n",
              " 'place clean food oh stal',\n",
              " 'chicken dishes ok beef like shoe leath',\n",
              " 'service beyond bad',\n",
              " 'happi',\n",
              " 'tasted like dirt',\n",
              " 'one places phoenix would definately go back',\n",
              " 'block amaz',\n",
              " 'close house low key non fancy affordable prices good food',\n",
              " 'hot sour egg flower soups absolutely star',\n",
              " 'sashimi poor quality soggy tasteless',\n",
              " 'great time family dinner sunday night',\n",
              " 'food tasty say real traditional hunan styl',\n",
              " 'bother slow servic',\n",
              " 'flair bartenders absolutely amaz',\n",
              " 'frozen margaritas way sugary tast',\n",
              " 'good ordered twic',\n",
              " 'nutshell restaraunt smells like combination dirty fish market sew',\n",
              " 'girlfriend veal bad',\n",
              " 'unfortunately good',\n",
              " 'pretty satifying experi',\n",
              " 'join club get awesome offers via email',\n",
              " 'perfect someone likes beer ice cold case even cold',\n",
              " 'bland flavorless good way describing barely tepid meat',\n",
              " 'chains fan beat place easili',\n",
              " 'nachos must',\n",
              " 'coming back',\n",
              " 'many words say place everything pretty wel',\n",
              " 'staff super nice quick even crazy crowds downtown juries lawyers court staff',\n",
              " 'great atmosphere friendly fast servic',\n",
              " 'received pita huge lot meat thumb',\n",
              " 'food arrives meh',\n",
              " 'paying hot dog fries looks like came kid meal wienerschnitzel idea good m',\n",
              " 'classic maine lobster roll fantast',\n",
              " 'brother law works mall ate day guess sick night',\n",
              " 'good going review place twice hereas tribute place tribute event held last night',\n",
              " 'chips salsa really good salsa fresh',\n",
              " 'place great',\n",
              " 'mediocre food',\n",
              " 'get inside impressed plac',\n",
              " 'super pissd',\n",
              " 'service super friendli',\n",
              " 'sad little vegetables overcook',\n",
              " 'place nice surpris',\n",
              " 'golden crispy delici',\n",
              " 'high hopes place since burgers cooked charcoal grill unfortunately taste fell flat way flat',\n",
              " 'could eat bruschetta day devin',\n",
              " 'single employee came see ok even needed water refill finally served us food',\n",
              " 'lastly mozzarella sticks best thing ord',\n",
              " 'first time ever came amazing experience still tell people awesome duck',\n",
              " 'server negligent needs made us feel unwelcome would suggest plac',\n",
              " 'service terrible though',\n",
              " 'place overpriced consistent boba really overpr',\n",
              " 'pack',\n",
              " 'love plac',\n",
              " 'say desserts yummi',\n",
              " 'food terr',\n",
              " 'seasonal fruit fresh white peach pure',\n",
              " 'kept getting worse worse officially don',\n",
              " 'place honestly blown',\n",
              " 'definitely would eat',\n",
              " 'waste money',\n",
              " 'love put food nice plastic containers opposed cramming little paper takeout box',\n",
              " 'cr pe delicate thin moist',\n",
              " 'awful servic',\n",
              " 'ever go',\n",
              " 'food quality horr',\n",
              " 'price think place would much rather gon',\n",
              " 'service fair best',\n",
              " 'love sushi found kabuki priced hip servic',\n",
              " 'favor stay away dish',\n",
              " 'poor servic',\n",
              " 'one table thought food average worth wait',\n",
              " 'best service food ever maria server good friendly made day',\n",
              " 'excel',\n",
              " 'paid bill tip felt server terrible job',\n",
              " 'lunch great experi',\n",
              " 'never bland food surprised considering article read focused much spices flavor',\n",
              " 'food way overpriced portions fucking smal',\n",
              " 'recently tried caballero back every week sinc',\n",
              " 'bucks head really expect better food',\n",
              " 'food came good pac',\n",
              " 'ate twice last visit especially enjoyed salmon salad',\n",
              " 'back',\n",
              " 'could believe dirty oyst',\n",
              " 'place deserves star',\n",
              " 'would recommend plac',\n",
              " 'fact going round stars awesom',\n",
              " 'disbelief dish qualified worst version foods ever tast',\n",
              " 'bad day low tolerance rude customer service people job nice polite wash dishes otherwis',\n",
              " 'potatoes great biscuit',\n",
              " 'probably would go',\n",
              " 'flavorful perfect amount heat',\n",
              " 'price reasonable service great',\n",
              " 'wife hated meal coconut shrimp friends really enjoy meals eith',\n",
              " 'fella got huevos rancheros look app',\n",
              " 'went happy hour great list win',\n",
              " 'may say buffet pricey think get pay place getting quite lot',\n",
              " 'probably coming back',\n",
              " 'worst food servic',\n",
              " 'place pretty good nice little vibe restaur',\n",
              " 'talk great customer service course back',\n",
              " 'hot dishes hot cold dishes close room temp watched staff prepare food bare hands gloves everything deep fried oil',\n",
              " 'love fries bean',\n",
              " 'always pleasure d',\n",
              " 'plethora salads sandwiches everything tried gets seal approv',\n",
              " 'place awesome want something light healthy summ',\n",
              " 'sushi strip place go',\n",
              " 'service great even manager came helped t',\n",
              " 'feel dining room college cooking course high class dining service slow best',\n",
              " 'started review two stars editing give on',\n",
              " 'worst sushi ever eat besides costco',\n",
              " 'excellent restaurant highlighted great service unique menu beautiful set',\n",
              " 'boyfriend sat bar completely delightful experi',\n",
              " 'weird vibe own',\n",
              " 'hardly meat',\n",
              " 'better bagels grocery stor',\n",
              " 'go place gyro',\n",
              " 'love owner chef one authentic japanese cool dud',\n",
              " 'burgers good pizza used amazing doughy flavorless',\n",
              " 'found six inch long piece wire salsa',\n",
              " 'service terrible food mediocr',\n",
              " 'definately enjoy',\n",
              " 'ordered albondigas soup warm tasted like tomato soup frozen meatbal',\n",
              " 'three different occasions asked well done medium well three times got bloodiest piece meat pl',\n",
              " 'two bites refused eat anymor',\n",
              " 'service extremely slow',\n",
              " 'minutes wait got t',\n",
              " 'seriously killer hot chai latt',\n",
              " 'allergy warnings menu waitress absolutely clue meals contain peanut',\n",
              " 'boyfriend tried mediterranean chicken salad fell lov',\n",
              " 'rotating beers tap also highlight plac',\n",
              " 'pricing bit concern mellow mushroom',\n",
              " 'worst thai ev',\n",
              " 'stay vegas must get breakfast least',\n",
              " 'want first say server great perfect servic',\n",
              " 'pizza selections good',\n",
              " 'strawberry tea good',\n",
              " 'highly unprofessional rude loyal patron',\n",
              " 'overall great experi',\n",
              " 'spend money elsewher',\n",
              " 'regular toasted bread equally satisfying occasional pats butter mmmm',\n",
              " 'buffet bellagio far anticip',\n",
              " 'drinks weak peopl',\n",
              " 'order correct',\n",
              " 'also feel like chips bought made hous',\n",
              " 'disappointing dinner went elsewhere dessert',\n",
              " 'chips sals amaz',\n",
              " 'return',\n",
              " 'new fav vegas buffet spot',\n",
              " 'seriously cannot believe owner many unexperienced employees running around like chickens heads cut',\n",
              " 'sad',\n",
              " 'felt insulted disrespected could talk judge another human lik',\n",
              " 'call steakhouse properly cook steak understand',\n",
              " 'impressed concept food',\n",
              " 'thing crazy guacamole like pur ',\n",
              " 'really nothing postinos hope experience bett',\n",
              " 'got food poisoning buffet',\n",
              " 'brought fresh batch fries thinking yay something warm',\n",
              " 'hilarious yummy christmas eve dinner remember biggest fail entire trip u',\n",
              " 'needless say going back anytime soon',\n",
              " 'place disgust',\n",
              " 'every time eat see caring teamwork professional degre',\n",
              " 'ri style calamari jok',\n",
              " 'however much garlic fondue barely ed',\n",
              " 'could barely stomach meal complain business lunch',\n",
              " 'bad lost heart finish',\n",
              " 'also took forever bring us check ask',\n",
              " 'ones make scene restaurants get definitely lost love on',\n",
              " 'disappointing experi',\n",
              " 'food par denny say good',\n",
              " 'want wait mediocre food downright terrible service plac',\n",
              " 'waaaaaayyyyyyyyyy rated say',\n",
              " 'going back',\n",
              " 'place fairly clean food simply worth',\n",
              " 'place lacked styl',\n",
              " 'sangria half glass wine full ridicul',\n",
              " 'bother com',\n",
              " 'meat pretty dry sliced brisket pulled pork',\n",
              " 'building seems pretty neat bathroom pretty trippy eat',\n",
              " 'equally aw',\n",
              " 'probably hurry go back',\n",
              " 'slow seating even reserv',\n",
              " 'good stretch imagin',\n",
              " 'cashew cream sauce bland vegetables undercook',\n",
              " 'chipolte ranch dipping sause tasteless seemed thin watered heat',\n",
              " 'bit sweet really spicy enough lacked flavor',\n",
              " 'disappoint',\n",
              " 'place horrible way overpr',\n",
              " 'maybe vegetarian fare twice thought average best',\n",
              " 'busy know',\n",
              " 'tables outside also dirty lot time workers always friendly helpful menu',\n",
              " 'ambiance feel like buffet setting douchey indoor garden tea biscuit',\n",
              " 'con spotty servic',\n",
              " 'fries hot neither burg',\n",
              " 'came back cold',\n",
              " 'food came disappointment ensu',\n",
              " 'real disappointment wait',\n",
              " 'husband said rude even apologize bad food anyth',\n",
              " 'reason eat would fill night binge drinking get carbs stomach',\n",
              " 'insults profound deuchebaggery go outside smoke break serving solidifi',\n",
              " 'someone orders two tacos think may part customer service ask combo ala cart',\n",
              " 'quite disappointed although blame needs placed door',\n",
              " 'rave reviews wait eat disappoint',\n",
              " 'del taco pretty nasty avoided poss',\n",
              " 'hard make decent hamburg',\n",
              " 'like',\n",
              " 'hell go back',\n",
              " 'gotten much better service pizza place next door services received restaur',\n",
              " 'know big deal place back ya',\n",
              " 'immediately said wanted talk manager want talk guy shots fireball behind bar',\n",
              " 'ambiance much bett',\n",
              " 'unfortunately set us disapppointment entre',\n",
              " 'food good',\n",
              " 'servers suck wait correction server heimer suck',\n",
              " 'happened next pretty put',\n",
              " 'bad cause know family owned really wanted like plac',\n",
              " 'overpriced get',\n",
              " 'vomited bathroom mid lunch',\n",
              " 'kept looking time soon become minutes yet still food',\n",
              " 'places eat circumstances would ever return tops list',\n",
              " 'started tuna sashimi brownish color obviously fresh',\n",
              " 'food averag',\n",
              " 'sure beat nachos movies would expect little bit coming restaur',\n",
              " 'ha long bay bit flop',\n",
              " 'problem charge sandwich bigger subway sub offers better amount veget',\n",
              " 'shrimp unwrapped live mile brushfire literally ice cold',\n",
              " 'lacked flavor seemed undercooked dri',\n",
              " 'really impressive place clos',\n",
              " 'would avoid place staying mirag',\n",
              " 'refried beans came meal dried crusty food bland',\n",
              " 'spend money time place els',\n",
              " 'lady table next us found live green caterpillar salad',\n",
              " 'presentation food aw',\n",
              " 'tell disappoint',\n",
              " 'think food flavor texture lack',\n",
              " 'appetite instantly gon',\n",
              " 'overall impressed would go back',\n",
              " 'whole experience underwhelming think go ninja sushi next tim',\n",
              " 'wasted enough life poured salt wound drawing time took bring check']"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()"
      ],
      "metadata": {
        "id": "pTNTh7Q-yZiT"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.fit_transform(corpus).toarray().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA5BZ7a5yeuf",
        "outputId": "caf88f99-59d7-4e0b-8a44-ae18dee4e60a"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1994)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = data['Liked']"
      ],
      "metadata": {
        "id": "Eu3kbBzoyw6D"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "446cpZJ-zHQi"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "anDQJDC1zY3C"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SBmDkdLzrRD",
        "outputId": "3d4281e8-ed89-4d81-92b5-c7adc316f7cf"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(670, 1994)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR5gOuN3z7rL",
        "outputId": "b20aa358-962a-4734-bc2d-ba3ab78700c2"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(330, 1994)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Model Training**"
      ],
      "metadata": {
        "id": "ZcbeAmw30ALr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "L69NhwXW0FYa"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MultinomialNB()"
      ],
      "metadata": {
        "id": "X85EyLKT0aRS"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "oGSTdE3n0eAb",
        "outputId": "20a50d16-1adb-420d-d67c-09e9c73ca938"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Prediction**"
      ],
      "metadata": {
        "id": "drHz1bMN0tZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rypxxg80y-j",
        "outputId": "912a0e11-df5e-4c4e-86a9-7429ae40d1e5"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
              "       1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
              "       0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
              "       1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd95Y9hI1G1C",
        "outputId": "41fa82ae-fb27-4e73-a41a-a4bd0a219813"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
              "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
              "       0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
              "       0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
              "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "uK6aX4kO1YjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , accuracy_score , classification_report"
      ],
      "metadata": {
        "id": "3NHEMaTP1cTc"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test , y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuU5mquP2wy9",
        "outputId": "65789872-13e7-4727-f89c-4dab8eb501b6"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[116  57]\n",
            " [ 32 125]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test , y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxE5CjDK27XV",
        "outputId": "555806a1-56ab-4ee0-a281-8b9aad05c465"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7303030303030303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test , y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmaFddqe2_IU",
        "outputId": "3373a643-d7f0-4586-d92d-ea28b50e0f03"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.67      0.72       173\n",
            "           1       0.69      0.80      0.74       157\n",
            "\n",
            "    accuracy                           0.73       330\n",
            "   macro avg       0.74      0.73      0.73       330\n",
            "weighted avg       0.74      0.73      0.73       330\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Y8SV1fPOJpW"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "FYen0PK3P3iE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "0c6293a6-9f23-47ff-8dad-3eaee64c170e"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "6yYTsJifOXJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab8f09b-fbd6-4935-8284-01c91bc21e48"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7545454545454545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Named Entity Recognition**"
      ],
      "metadata": {
        "id": "1mxA8G1LwxVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "KiEciwCpw46G"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/NER.csv\",encoding = 'latin1')\n",
        "data = data.fillna(method = 'ffill')\n",
        "data.head()\n",
        "# data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "l91GFZRcw6cu",
        "outputId": "e09b12df-fd2d-47ac-8479-c2b16e954e91"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/NER.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-30f04396bef4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/NER.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ffill'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# data.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/NER.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.nunique()"
      ],
      "metadata": {
        "id": "tOWSCRVKw8pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase words\n",
        "data['Word'] = data['Word'].str.lower()"
      ],
      "metadata": {
        "id": "s486eBJVw-EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(set(data[\"Word\"].values))\n",
        "words.append(\"ENDPAD\")\n",
        "num_words = len(words)\n",
        "num_words"
      ],
      "metadata": {
        "id": "IHEjKLXYw-Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_tag = list(set(data[\"Tag\"].values))\n",
        "# words_tag.append(\"ENDPAD\")\n",
        "num_words_tag = len(words_tag)\n",
        "num_words_tag"
      ],
      "metadata": {
        "id": "Ww2dlS4lw_eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words,num_words_tag\n",
        "group = data.groupby(data[\"Sentence #\"])\n",
        "# group.groups"
      ],
      "metadata": {
        "id": "0ipI3XWwxAeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Get_sentence(object):\n",
        "    def __init__(self,data):\n",
        "        self.n_sent=1\n",
        "        self.data = data\n",
        "        agg_func = lambda s:[(w,p,t) for w,p,t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                     s[\"POS\"].values.tolist(),\n",
        "                                                     s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]"
      ],
      "metadata": {
        "id": "XLvQ2cZIxCRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getter = Get_sentence(data)\n",
        "sentence = getter.sentences\n",
        "sentence[0]"
      ],
      "metadata": {
        "id": "cyg9lRdnxDnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_idx = {w : i+1 for i ,w in enumerate(words)}\n",
        "tag_idx =  {t : i for i ,t in enumerate(words_tag)}"
      ],
      "metadata": {
        "id": "eBRP-m8ExEuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist([len(s) for s in sentence],bins= 50)\n",
        "plt.xlabel(\"Length of Sentences\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IcCGwzZjxF2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "max_len = 50\n",
        "X = [[word_idx[w[0]] for w in s] for s in sentence]\n",
        "X = pad_sequences(maxlen = max_len,sequences = X,padding = 'post',value = num_words-1)\n",
        "y = [[tag_idx[w[2]] for w in s] for s in sentence]\n",
        "y = pad_sequences(maxlen = max_len,sequences = y,padding = 'post',value = tag_idx['O'])\n",
        "y = [to_categorical(i,num_classes = num_words_tag) for i in  y]"
      ],
      "metadata": {
        "id": "bAW_IhUVxOQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state=1)"
      ],
      "metadata": {
        "id": "EFK8Oyu3xPr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model,Input\n",
        "from tensorflow.keras.layers import LSTM,Embedding,Dense\n",
        "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D,Bidirectional"
      ],
      "metadata": {
        "id": "9VbHq4_nxQo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_word = Input(shape = (max_len,))\n",
        "model = Embedding(input_dim = num_words,output_dim = max_len,input_length = max_len)(input_word)\n",
        "model = SpatialDropout1D(0.1)(model)\n",
        "model = Bidirectional(LSTM(units=100,return_sequences = True, recurrent_dropout = 0.1))(model)\n",
        "out = TimeDistributed(Dense(num_words_tag,activation = 'softmax'))(model)\n",
        "model = Model(input_word,out)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "sYxiJR_OxRff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "eYm5qIp2xSmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "# from livelossplot import PlotLossesKeras\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy',patience =2,verbose = 0,mode = 'max',restore_best_weights = False)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,np.array(y_train),\n",
        "    validation_split =0.2,\n",
        "    batch_size = 64,\n",
        "    epochs = 5,\n",
        "    verbose =1\n",
        ")"
      ],
      "metadata": {
        "id": "9qxYsraQxTlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test,np.array(y_test))"
      ],
      "metadata": {
        "id": "ctmXEBbnxUwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "input_word = Input(shape=(max_len,))\n",
        "model = Embedding(input_dim=num_words, output_dim=max_len, input_length=max_len)(input_word)\n",
        "model = SpatialDropout1D(0.1)(model)\n",
        "model = Bidirectional(GRU(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        "out = TimeDistributed(Dense(num_words_tag, activation='softmax'))(model)\n",
        "model = Model(input_word, out)\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "uMn9GuNPF3ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy',patience =2,verbose = 0,mode = 'max',restore_best_weights = False)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,np.array(y_train),\n",
        "    validation_split =0.2,\n",
        "    batch_size = 64,\n",
        "    epochs = 5,\n",
        "    verbose =1\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wX3kvXxzF5po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test,np.array(y_test))"
      ],
      "metadata": {
        "id": "O3-wIQFxKY8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user input for sentence\n",
        "input_sentence = input(\"Enter a sentence to evaluate: \")\n",
        "\n",
        "# Tokenize the input sentence\n",
        "input_tokens = input_sentence.split()\n",
        "\n",
        "# Convert tokens to lowercase\n",
        "input_tokens_lower = [token.lower() for token in input_tokens]\n",
        "\n",
        "# Convert tokens to indices using the word_idx mapping\n",
        "input_indices = [word_idx.get(token, word_idx.get(token.lower(), 0)) for token in input_tokens]\n",
        "\n",
        "# Pad or truncate the input sequence to match the expected length\n",
        "input_indices = input_indices[:max_len] + [0] * (max_len - len(input_indices))\n",
        "\n",
        "# Make prediction for the input sentence\n",
        "prediction = model.predict(np.array([input_indices]))\n",
        "predicted_labels = np.argmax(prediction, axis=-1)[0]\n",
        "\n",
        "# Convert predicted labels to words\n",
        "predicted_tags = [words_tag[idx] for idx in predicted_labels]\n",
        "\n",
        "# Print the predicted tags for the input sentence\n",
        "print(\"Predicted tags for the input sentence:\")\n",
        "for token, tag in zip(input_tokens, predicted_tags[:len(input_tokens)]):\n",
        "    print(f\"{token}: {tag}\")\n"
      ],
      "metadata": {
        "id": "L7ii8QuuxVsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Summerization**"
      ],
      "metadata": {
        "id": "72hLqrQMEX8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libiray\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation"
      ],
      "metadata": {
        "id": "YUdxwcPKmZuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalization (Data Preprocessing)\n",
        "def cleanText(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('[^\\w\\s]','',text)\n",
        "\n",
        "    #Remove spaces at the beginning and at the end of the string\n",
        "    text.strip()\n",
        "\n",
        "    txt=[]\n",
        "    for w in text.split():\n",
        "        stemWord = porter.stem(w)\n",
        "        txt.append(stemWord)\n",
        "    txt = ' '.join(txt)\n",
        "    return txt"
      ],
      "metadata": {
        "id": "O4RaSL3OQv6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the datasets (training - testing - validation)\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/train.csv')\n",
        "val_df = pd.read_csv('/content/val.csv')\n",
        "# train_df.head()\n",
        "train_df.shape"
      ],
      "metadata": {
        "id": "QKDooccbQzPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine data from the three CSV files into a single DataFrame\n",
        "pre = pd.DataFrame()\n",
        "pre['text'] = pd.concat([train_df['document'], val_df['document'], test_df['document']], ignore_index=True)\n",
        "pre['summary'] = pd.concat([train_df['summary'], val_df['summary'], test_df['summary']], ignore_index=True)\n",
        "pre.head()"
      ],
      "metadata": {
        "id": "EAeuSqYBQ22D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre['text'] = pre['text'].apply(cleanText)\n",
        "pre['summary'] = pre['summary'].apply(cleanText)"
      ],
      "metadata": {
        "id": "3Z9WRW7gQ4Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre.shape\n",
        "# pre.head()"
      ],
      "metadata": {
        "id": "sY-efM68Q5mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for null values\n",
        "pre.isnull().sum()"
      ],
      "metadata": {
        "id": "gmmbFL5GQ60U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check on the most number of word in text\n",
        "# Check how much % of text have 0-60 words\n",
        "cnt = 0\n",
        "for i in pre['text']:\n",
        "    if len(i.split()) <= 70:\n",
        "        cnt = cnt + 1\n",
        "print(cnt / len(pre['text']))"
      ],
      "metadata": {
        "id": "vuCy0IuEQ750"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check on the most number of word in summary\n",
        "# Check how much % of summary have 0-20 words\n",
        "cnt = 0\n",
        "for i in pre['summary']:\n",
        "    if len(i.split()) <= 20:\n",
        "        cnt = cnt + 1\n",
        "print(cnt / len(pre['summary']))"
      ],
      "metadata": {
        "id": "RErr7T65Q85c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model to summarize the text between 0-20 words for Summary and 0-70 words for Text\n",
        "max_text_len = 70\n",
        "max_summary_len = 20"
      ],
      "metadata": {
        "id": "wvQ1nY95Q988"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the Summaries and Text which fall below max length\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "cleaned_text = np.array(pre['text'])\n",
        "cleaned_summary = np.array(pre['summary'])\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "for i in range(len(train_df)):\n",
        "    if len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_text[i].split()) <= max_text_len:\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "\n",
        "post_pre = pd.DataFrame({'text': short_text,'summary': short_summary})\n",
        "\n",
        "post_pre.head(2)"
      ],
      "metadata": {
        "id": "otBhIUJtRB8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add sostok(start of the sequence) and eostok(end of the sequence)\n",
        "\n",
        "post_pre['summary'] = post_pre['summary'].apply(lambda x: 'sostok ' + x \\\n",
        "        + ' eostok')\n",
        "\n",
        "post_pre.head(2)"
      ],
      "metadata": {
        "id": "5KjRMj59RDEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and test data chunks.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train ,x_test, y_train, y_test = train_test_split(\n",
        "    np.array(post_pre[\"text\"]),\n",
        "    np.array(post_pre[\"summary\"]),\n",
        "    test_size=0.1,\n",
        "    random_state=0,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "gcVI3vkkREI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text to get the vocab count\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Prepare a tokenizer on training data\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "\n",
        "# print(x_tokenizer.word_index)"
      ],
      "metadata": {
        "id": "7SQ-0ccqRFLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(x_tokenizer.word_index)\n",
        "x_tokenizer.word_counts.items()"
      ],
      "metadata": {
        "id": "m07ApieORGX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the percentage of occurrence of rare words (say, occurring less than 5 times) in the text.\n",
        "\n",
        "cnt = 0\n",
        "tot_cnt = 0\n",
        "\n",
        "for key, value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt = tot_cnt + 1\n",
        "    if value < 5:\n",
        "        cnt = cnt + 1\n",
        "\n",
        "print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)"
      ],
      "metadata": {
        "id": "OIa1rKfsRIm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt)\n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "x_train_seq = x_tokenizer.texts_to_sequences(x_train)\n",
        "x_test_seq = x_tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Pad zero upto maximum length\n",
        "x_train = pad_sequences(x_train_seq,  maxlen=max_text_len, padding='post')\n",
        "x_test = pad_sequences(x_test_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "x_voc = x_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))"
      ],
      "metadata": {
        "id": "ma6DWDRURJ1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a tokenizer on testing data\n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "cnt = 0\n",
        "tot_cnt = 0\n",
        "\n",
        "for key, value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt = tot_cnt + 1\n",
        "    if value < 5:\n",
        "        cnt = cnt + 1\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
        "\n",
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "y_train_seq = y_tokenizer.texts_to_sequences(y_train)\n",
        "y_test_seq = y_tokenizer.texts_to_sequences(y_test)\n",
        "\n",
        "# Pad zero upto maximum length\n",
        "y_train = pad_sequences(y_train_seq, maxlen=max_summary_len, padding='post')\n",
        "y_test = pad_sequences(y_test_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "y_voc = y_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
      ],
      "metadata": {
        "id": "0QmIifXoRLJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "9SAx2Z1LRMdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 300\n",
        "embedding_dim = 200\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len, ))\n",
        "\n",
        "# Embedding layer\n",
        "enc_emb = Embedding(x_voc, embedding_dim, trainable=True)(encoder_inputs)\n",
        "\n",
        "# Encoder LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
        "\n",
        "# Encoder LSTM 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# Encoder LSTM 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using encoder_states as the initial state\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "\n",
        "# Embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
        "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
        "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "P_AYQKiVRNds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "WA_N93eyRO5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
      ],
      "metadata": {
        "id": "pr1p4xPoRP9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train[0][0:-7]\n",
        "# y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:]\n",
        "x_train.dtype"
      ],
      "metadata": {
        "id": "h5ayZGKDRRCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xt = np.array(x_train)\n",
        "# yt = np.array(y_train)\n",
        "\n",
        "# yt = np.reshape(y_train.shape[0], y_train.shape[1], 1)\n",
        "\n",
        "y_train.shape\n",
        "# gg = np.array([[1,2,3],])\n",
        "# gg.shape[1]\n",
        "\n",
        "# nn = np.reshape(gg, 3)\n",
        "# nn\n",
        "# x_train, y_train[0][0:-7]"
      ],
      "metadata": {
        "id": "JE1vw4bvRSN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    [x_train, y_train[:, :-1]],\n",
        "    y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:, 1:],\n",
        "    epochs=4,\n",
        "    callbacks=[es],\n",
        "    batch_size=128,\n",
        "    validation_data=([x_test, y_test[:, :-1]],y_test.reshape(y_test.shape[0], y_test.shape[1], 1)[:, 1:]),\n",
        "    )"
      ],
      "metadata": {
        "id": "UXgbR0rZRT60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_target_word_index = y_tokenizer.index_word\n",
        "reverse_source_word_index = x_tokenizer.index_word\n",
        "target_word_index = y_tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "6Qu5-WI7RVQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Models\n",
        "\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
        "                      state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
        "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
        "                      decoder_state_input_h, decoder_state_input_c],\n",
        "                      [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "E52wglcTRWXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
        "                + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "        if sampled_token != 'eostok':\n",
        "            decoded_sentence += ' ' + sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find the stop word.\n",
        "        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n",
        "            >= max_summary_len - 1:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        (e_h, e_c) = (h, c)\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "0_hKG3OORXe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seq2summary() and seq2text() which convert numeric-representation to string-representation of summary and text respectively.\n",
        "# To convert sequence to summary\n",
        "def seq2summary(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0 and i != target_word_index['sostok'] and i \\\n",
        "            != target_word_index['eostok']:\n",
        "            newString = newString + reverse_target_word_index[i] + ' '\n",
        "\n",
        "    return newString\n",
        "\n",
        "\n",
        "# To convert sequence to text\n",
        "def seq2text(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0:\n",
        "            newString = newString + reverse_source_word_index[i] + ' '\n",
        "\n",
        "    return newString"
      ],
      "metadata": {
        "id": "yp8s3zTJRYn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 19):\n",
        "    print ('Review:', seq2text(x_train[i]))\n",
        "    print ('Original summary:', seq2summary(y_train[i]))\n",
        "    print ('Predicted summary:', decode_sequence(x_train[i].reshape(1,max_text_len)))\n",
        "    print ('\\n')"
      ],
      "metadata": {
        "id": "P85PkmxURaAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Machine Translation**"
      ],
      "metadata": {
        "id": "dortdoelEfOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow-text>=2.11\";\n",
        "!pip install einops;"
      ],
      "metadata": {
        "id": "xVP-NHn6FsNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, GRU, Conv1D, GlobalMaxPooling1D, TimeDistributed, Dropout, RepeatVector, Concatenate\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pickle\n",
        "import os\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import tensorflow_text as tf_text\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "axOWsaN1Fs3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv(\"/content/ara_eng.txt\", delimiter=\"\\t\", names=[\"english\", \"arabic\"])"
      ],
      "metadata": {
        "id": "y6To_XvPFumG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1"
      ],
      "metadata": {
        "id": "7gdzcJBwFxL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.read_csv(\"/content/ara_eng.txt\", delimiter=\"\\t\", names=['english', 'arabic', 'CC'])"
      ],
      "metadata": {
        "id": "jSM2z50bF0mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.drop('CC', inplace=True, axis=1)\n",
        "data2"
      ],
      "metadata": {
        "id": "ocqyAgVHF2w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([data1, data2], ignore_index=True)"
      ],
      "metadata": {
        "id": "IpQ4-TrdF3RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "-dzFUG2hF48u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_duplicates = data.duplicated().sum()\n",
        "print(f\"Number of Duplicate Rows: {num_duplicates}\")"
      ],
      "metadata": {
        "id": "2Mm2C20EF6D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates from data\n",
        "data = data.drop_duplicates()"
      ],
      "metadata": {
        "id": "VCJkM6n9F7_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "mfi1dNdCF9nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.subplots as sp\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "input_lengths = [len(seq.split()) for seq in data['english']]\n",
        "output_lengths = [len(seq.split()) for seq in data['arabic']]\n",
        "\n",
        "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=('English Sentence Lengths', 'Arabic Sentence Lengths'))\n",
        "\n",
        "hist_input = go.Histogram(x=input_lengths, nbinsx=50, name='English')\n",
        "hist_output = go.Histogram(x=output_lengths, nbinsx=50, name='Arabic')\n",
        "\n",
        "fig.add_trace(hist_input, row=1, col=1)\n",
        "fig.add_trace(hist_output, row=1, col=2)\n",
        "\n",
        "fig.update_layout(showlegend=False, title_text='Distribution of Sentence Lengths')\n",
        "fig.update_xaxes(title_text='Sentence Length', row=1, col=1)\n",
        "fig.update_xaxes(title_text='Sentence Length', row=1, col=2)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "lugDNO55F-hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "unique_words_input = len(set(word for seq in data['english'] for word in seq.split()))\n",
        "unique_words_output = len(set(word for seq in data['arabic'] for word in seq.split()))\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Bar(x=['English'], y=[unique_words_input], name='English'))\n",
        "fig.add_trace(go.Bar(x=['Arabic'], y=[unique_words_output], name='Arabic'))\n",
        "\n",
        "fig.update_layout(title_text='Total Number of Unique Words in Each Language', barmode='group', xaxis_title='Language', yaxis_title='Total Unique Words')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "rcPeoRTRF_xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove punctuation from a sentence\n",
        "def remove_punctuation(sentence):\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    return sentence.translate(translator)\n",
        "\n",
        "# Combine all English sentences into a single string and remove punctuation\n",
        "all_english_sentences = ' '.join(data['english'])\n",
        "# all_english_sentences = remove_punctuation(all_english_sentences)\n",
        "\n",
        "# Combine all Arabic sentences into a single string and remove punctuation\n",
        "all_arabic_sentences = ' '.join(data['arabic'])\n",
        "# all_arabic_sentences = remove_punctuation(all_arabic_sentences)\n",
        "\n",
        "# Count word frequencies in English sentences\n",
        "english_word_counts = Counter(all_english_sentences.lower().split())\n",
        "\n",
        "# Count word frequencies in Arabic sentences\n",
        "arabic_word_counts = Counter(all_arabic_sentences.split())\n",
        "\n",
        "target_freq = 1\n",
        "\n",
        "# Display the number of words that occur more than 100 times for English\n",
        "print(f\"English Words with Frequency equals {target_freq}:\")\n",
        "total = 0\n",
        "for word, count in english_word_counts.items():\n",
        "    if count == target_freq:\n",
        "        total += 1\n",
        "#         print(f\"{word}: {count}\")\n",
        "print(total)\n",
        "\n",
        "# Display the number of words that occur more than 100 times for Arabic\n",
        "print(f\"Arabic Words with Frequency equals {target_freq}:\")\n",
        "total = 0\n",
        "for word, count in arabic_word_counts.items():\n",
        "    if count == target_freq:\n",
        "        total += 1\n",
        "#         print(f\"{word}: {count}\")\n",
        "print(total)\n"
      ],
      "metadata": {
        "id": "lY9Yr0GdGB__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Count the number of entries with only less than 6 words for English\n",
        "english_one_word_count = data['english'].apply(lambda x: 1 if len(str(x).split()) < 6 else 0).sum()\n",
        "\n",
        "# Count the number of entries with only less than 6 words for Arabic\n",
        "arabic_one_word_count = data['arabic'].apply(lambda x: 1 if len(str(x).split()) < 6 else 0).sum()\n",
        "\n",
        "# Create a DataFrame for the plot\n",
        "info = {'Language': ['English', 'Arabic'], 'Entries with less than 6 words': [english_one_word_count, arabic_one_word_count]}\n",
        "df_plot = pd.DataFrame(info)\n",
        "\n",
        "# Plot the graph using Plotly Express\n",
        "fig = px.bar(df_plot, x='Language', y='Entries with less than 6 words', text='Entries with less than 6 words',\n",
        "             title='Number of Entries with less than 6 words',\n",
        "             labels={'Entries with less than 6 words': 'Number of Entries'})\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "XBzeJxErGIbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "GqcCnRfqGMU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for padding and OOV tokens\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "max_sequence_length = 10\n",
        "\n",
        "\n",
        "# Tokenize English sentences\n",
        "english_tokenizer = Tokenizer(oov_token=oov_tok,\n",
        "                              lower=True)\n",
        "english_tokenizer.fit_on_texts(train_df['english'])\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "\n",
        "# Tokenize English sentences\n",
        "arabic_tokenizer = Tokenizer(oov_token=oov_tok,\n",
        "                             lower=True)\n",
        "arabic_tokenizer.fit_on_texts(train_df['arabic'])\n",
        "arabic_vocab_size = len(arabic_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "rc8SHqE6GOFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tokenizers\n",
        "directory = './tokenizer/'\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "with open('./tokenizer/simple_english_tokenizer.pickle', 'wb+') as handle:\n",
        "    pickle.dump(english_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('./tokenizer/simple_arabic_tokenizer.pickle', 'wb+') as handle:\n",
        "    pickle.dump(arabic_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "AlhfTI1kGPYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text sequences to integer sequences\n",
        "train_input_sequences = english_tokenizer.texts_to_sequences(train_df['english'])\n",
        "train_output_sequences = arabic_tokenizer.texts_to_sequences(train_df['arabic'])\n",
        "\n",
        "test_input_sequences = english_tokenizer.texts_to_sequences(test_df['english'])\n",
        "test_output_sequences = arabic_tokenizer.texts_to_sequences(test_df['arabic'])\n"
      ],
      "metadata": {
        "id": "rddi8RrRGQj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to have the same length\n",
        "train_input_sequences = pad_sequences(train_input_sequences, maxlen=max_sequence_length, padding=padding_type, truncating=trunc_type)\n",
        "train_output_sequences = pad_sequences(train_output_sequences, maxlen=max_sequence_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "test_input_sequences = pad_sequences(test_input_sequences, maxlen=max_sequence_length, padding=padding_type, truncating=trunc_type)\n",
        "test_output_sequences = pad_sequences(test_output_sequences, maxlen=max_sequence_length, padding=padding_type, truncating=trunc_type)"
      ],
      "metadata": {
        "id": "8VX-Cd5pGT1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the tokenizers"
      ],
      "metadata": {
        "id": "8PrxCCItGXq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_tokenizer.sequences_to_texts(train_input_sequences[0:5])"
      ],
      "metadata": {
        "id": "Y-HqxMpZGZLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_tokenizer.sequences_to_texts(train_output_sequences[:5])"
      ],
      "metadata": {
        "id": "WGaI-1HRGaZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the datasets"
      ],
      "metadata": {
        "id": "DfDuPeTZGdTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256"
      ],
      "metadata": {
        "id": "tgKkAmAmGejN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_train = np.random.uniform(size=(len(train_input_sequences),)) < 0.9\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_input_sequences[is_train], train_output_sequences[is_train])).batch(BATCH_SIZE)\n",
        "validation_ds = tf.data.Dataset.from_tensor_slices((train_input_sequences[~is_train], train_output_sequences[~is_train])).batch(BATCH_SIZE)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_input_sequences, test_output_sequences)).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "SZuU_YbnGk2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkpoint_callback(model_name):\n",
        "    return ModelCheckpoint(filepath=f\"./models/{model_name}.keras2\",\n",
        "                           save_weights_only=False,\n",
        "                           monitor='val_accuracy',\n",
        "                           mode='max',\n",
        "                           save_best_only=True)"
      ],
      "metadata": {
        "id": "Y-kkN-C7Glyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **LSTM Model**"
      ],
      "metadata": {
        "id": "K6uhY_vbGoTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BD_LSTM = tf.keras.Sequential([\n",
        "    Embedding(english_vocab_size, 256, mask_zero=True),\n",
        "    Bidirectional(LSTM(256)),\n",
        "    RepeatVector(max_sequence_length),\n",
        "    Bidirectional(LSTM(256, return_sequences=True)),\n",
        "    TimeDistributed(Dense(arabic_vocab_size, activation='softmax'))\n",
        "])"
      ],
      "metadata": {
        "id": "T1IajG_dG4P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BD_LSTM.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LQ5EglEFG5gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_history = BD_LSTM.fit(train_ds,\n",
        "        epochs=2,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=validation_ds)"
      ],
      "metadata": {
        "id": "TVXXH3E2G6sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU Model**"
      ],
      "metadata": {
        "id": "vVm_U6u_G-Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GRU = tf.keras.Sequential([\n",
        "    Embedding(english_vocab_size, 256, input_length=max_sequence_length, mask_zero=True),\n",
        "    GRU(256),\n",
        "    RepeatVector(max_sequence_length),\n",
        "    GRU(256, return_sequences=True),\n",
        "    Dense(arabic_vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "xo3ITMgaG-ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GRU.compile(optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "MSc_pnIBG_4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GRU_history = GRU.fit(train_ds,\n",
        "            epochs=2,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            validation_data=validation_ds,)"
      ],
      "metadata": {
        "id": "5u25cUAtHAzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "XIWuTfZAHDoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BD_LSTM_loss, BD_LSTM_accuracy = BD_LSTM.evaluate(test_input_sequences, test_output_sequences)\n",
        "GRU_loss, GRU_accuracy = GRU.evaluate(test_input_sequences, test_output_sequences)"
      ],
      "metadata": {
        "id": "AoZPvjz7HEO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for comparison\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['LSTM', 'GRU'],\n",
        "    'Loss': [BD_LSTM_loss, GRU_loss],\n",
        "    'Accuracy': [BD_LSTM_accuracy, GRU_accuracy]\n",
        "})\n",
        "\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "h8FCb4ziHFPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Tokenizer"
      ],
      "metadata": {
        "id": "ctrHstPqJzju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load context and target sentences\n",
        "def load_data(data):\n",
        "  context = np.array([context for context in data[\"english\"]])\n",
        "  target = np.array([target for target in data[\"arabic\"]])\n",
        "\n",
        "  return target, context"
      ],
      "metadata": {
        "id": "RSOW8eOyJvLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_raw, context_raw = load_data(data)\n",
        "print(context_raw[-1])"
      ],
      "metadata": {
        "id": "UNBrkVqIJ1aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_raw[-1])"
      ],
      "metadata": {
        "id": "m6Z9Hrr6J3B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(context_raw)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Train - Validation Split\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
        "\n",
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw, target_raw))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))\n",
        "\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))"
      ],
      "metadata": {
        "id": "p1ONnE3WJ4jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example_context_strings, example_target_strings in train_raw.take(1):\n",
        "  print(example_context_strings[:5][0].numpy().decode())\n",
        "  print()\n",
        "  print(example_target_strings[:5][0].numpy().decode())\n",
        "  break"
      ],
      "metadata": {
        "id": "v-y5l22WJ9AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-zا-ي.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "eobmjhIaJ-nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = tf.constant(target_raw[-1])\n",
        "\n",
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ],
      "metadata": {
        "id": "gdyVtzQ7J_vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 60000\n",
        "\n",
        "context_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)"
      ],
      "metadata": {
        "id": "_HZAQOPWKBLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "print(context_text_processor.get_vocabulary()[:10])\n",
        "print(context_text_processor.get_vocabulary()[-10:])\n",
        "print(len(context_text_processor.get_vocabulary()))"
      ],
      "metadata": {
        "id": "4EMoRgwOKCkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)\n",
        "\n",
        "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
        "print(target_text_processor.get_vocabulary()[:10])\n",
        "print(len(target_text_processor.get_vocabulary()))"
      ],
      "metadata": {
        "id": "aQT_Kg1BKDwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_tokens = context_text_processor(example_context_strings)\n",
        "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
        "tokens = context_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ],
      "metadata": {
        "id": "Z7t7nCRuKFF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(context, target):\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  target = target_text_processor(target)\n",
        "  targ_in = target[:,:-1].to_tensor()\n",
        "  targ_out = target[:,1:].to_tensor()\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "jIoNSSo1KGQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**"
      ],
      "metadata": {
        "id": "dP10UUpNKJJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UNITS = 1024"
      ],
      "metadata": {
        "id": "B6y8iiFcKJ7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "4uDwpQkXKR3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
        "                                               mask_zero=True)\n",
        "\n",
        "    # The RNN layer processes those vectors sequentially.\n",
        "    self.rnn = tf.keras.layers.Bidirectional(\n",
        "        merge_mode='sum',\n",
        "        layer=tf.keras.layers.GRU(units,\n",
        "                            # Return the sequence and state\n",
        "                            return_sequences=True,\n",
        "                            recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self, x):\n",
        "    # 2. The embedding layer looks up the embedding vector for each token.\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # 3. The GRU processes the sequence of embeddings.\n",
        "    x = self.rnn(x)\n",
        "\n",
        "    # 4. Returns the new sequence of embeddings.\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context"
      ],
      "metadata": {
        "id": "gP6AujmuKL64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "B4b0Qu3iKNeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "RZcHJS2gKcY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.word_to_id = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]')\n",
        "    self.id_to_word = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]',\n",
        "        invert=True)\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    self.units = units\n",
        "\n",
        "\n",
        "    # 1. The embedding layer converts token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
        "                                               units, mask_zero=True)\n",
        "\n",
        "    # 2. The RNN keeps track of what's been generated so far.\n",
        "    self.rnn = tf.keras.layers.GRU(units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = CrossAttention(units)\n",
        "\n",
        "    # 4. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "@Decoder.add_method\n",
        "def call(self,\n",
        "        context, x,\n",
        "        state=None,\n",
        "        return_state=False):\n",
        "\n",
        "  # 1. Lookup the embeddings\n",
        "  x = self.embedding(x)\n",
        "\n",
        "  # 2. Process the target sequence.\n",
        "  x, state = self.rnn(x, initial_state=state)\n",
        "\n",
        "  # 3. Use the RNN output as the query for the attention over the context.\n",
        "  x = self.attention(x, context)\n",
        "  self.last_attention_weights = self.attention.last_attention_weights\n",
        "\n",
        "  # Step 4. Generate logit predictions for the next token.\n",
        "  logits = self.output_layer(x)\n",
        "\n",
        "  if return_state:\n",
        "    return logits, state\n",
        "  else:\n",
        "    return logits\n",
        "\n",
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  embedded = self.embedding(start_tokens)\n",
        "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
        "\n",
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "  words = self.id_to_word(tokens)\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "  return result\n",
        "\n",
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "  logits, state = self(\n",
        "    context, next_token,\n",
        "    state = state,\n",
        "    return_state=True)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    logits = logits[:, -1, :]/temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "  # If a sequence produces an `end_token`, set it `done`\n",
        "  done = done | (next_token == self.end_token)\n",
        "  # Once a sequence is done it only produces 0-padding.\n",
        "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "  return next_token, done, state"
      ],
      "metadata": {
        "id": "2BULwx4vKO1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-Decoder Model"
      ],
      "metadata": {
        "id": "Y-WoBLsLKfzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, units,\n",
        "               context_text_processor,\n",
        "               target_text_processor):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(context_text_processor, units)\n",
        "    decoder = Decoder(target_text_processor, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self, inputs):\n",
        "    context, x = inputs\n",
        "    context = self.encoder(context)\n",
        "    logits = self.decoder(context, x)\n",
        "\n",
        "    #TODO(b/250038731): remove this\n",
        "    try:\n",
        "      # Delete the keras mask, so keras doesn't scale the loss+accuracy.\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits\n",
        "\n",
        "@Translator.add_method\n",
        "def translate(self,\n",
        "              texts, *,\n",
        "              max_length=50,\n",
        "              temperature=0.0):\n",
        "  # Process the input texts\n",
        "  context = self.encoder.convert_input(texts)\n",
        "  batch_size = tf.shape(texts)[0]\n",
        "\n",
        "  # Setup the loop inputs\n",
        "  tokens = []\n",
        "  attention_weights = []\n",
        "  next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    # Generate the next token\n",
        "    next_token, done, state = self.decoder.get_next_token(\n",
        "        context, next_token, done,  state, temperature)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    tokens.append(next_token)\n",
        "    attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Stack the lists of tokens and attention weights.\n",
        "  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "  result = self.decoder.tokens_to_text(tokens)\n",
        "  return result"
      ],
      "metadata": {
        "id": "zamOPWsrKhQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Masked Loss and Accuracy"
      ],
      "metadata": {
        "id": "mVpNmhr4Kwue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "3Nwp-AXoKyD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "MFFsAPhhKzWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Translator(UNITS, context_text_processor, target_text_processor)"
      ],
      "metadata": {
        "id": "1OytLPwOK051"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=masked_loss,\n",
        "              metrics=[masked_acc])"
      ],
      "metadata": {
        "id": "GuvgbTa1K2Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subword Tokenizer"
      ],
      "metadata": {
        "id": "ss_981TZK7on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import tensorflow_text as text"
      ],
      "metadata": {
        "id": "bwN-uQCgK82C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = 40000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")"
      ],
      "metadata": {
        "id": "9rDxsYTlK9H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ZwVHx8QKK-zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_en_raw = tf.constant(train_df['english'].values)\n",
        "train_ar_raw = tf.constant(train_df['arabic'].values)"
      ],
      "metadata": {
        "id": "VmqZIVbsLAGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_train = np.random.uniform(size=(len(train_en_raw),)) < 0.8\n",
        "BUFFER_SIZE = len(train_en_raw)\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Train And Validation Split\n",
        "train_ds = (tf.data.Dataset\n",
        "                    .from_tensor_slices((train_en_raw, train_ar_raw)))\n",
        "\n",
        "validation_ds = (tf.data.Dataset\n",
        "                    .from_tensor_slices((train_en_raw[~is_train], train_ar_raw[~is_train])))"
      ],
      "metadata": {
        "id": "8LZdjURuLA8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract English And Arabic sentences for tokenization\n",
        "train_en = train_ds.map(lambda en, ar: en)\n",
        "train_ar = train_ds.map(lambda en, ar: ar)"
      ],
      "metadata": {
        "id": "kbYT0OtWLCKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "metadata": {
        "id": "X8GDeGDzLDYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the tokenizers on the data\n",
        "ar_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_ar.prefetch(tf.data.AUTOTUNE),\n",
        "    **bert_vocab_args\n",
        ")\n",
        "\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.prefetch(tf.data.AUTOTUNE),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "id": "_71hGqguLEa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tokenizers' vocabulary\n",
        "directory = './tokenizer/subword/'\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "write_vocab_file('./tokenizer/subword/ar_vocab.txt', ar_vocab)\n",
        "write_vocab_file('./tokenizer/subword/en_vocab.txt', en_vocab)"
      ],
      "metadata": {
        "id": "140qSxNPLFeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create The Tokenizer"
      ],
      "metadata": {
        "id": "Lgo5QpznLIXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")"
      ],
      "metadata": {
        "id": "e6H-zsXKLG0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)\n",
        "\n",
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Drop the reserved tokens, except for \"[UNK]\".\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result\n",
        "\n",
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create the signatures for export:\n",
        "\n",
        "    # Include a tokenize signature for a batch of strings.\n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "    # Include `detokenize` and `lookup` signatures for:\n",
        "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "    #   * `RaggedTensors` with shape [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # These `get_*` methods take no arguments\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Merge the `word` and `word-piece` axes.\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)"
      ],
      "metadata": {
        "id": "UuhWEHOULJ1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.ar = CustomTokenizer(reserved_tokens, './tokenizer/subword/ar_vocab.txt')\n",
        "tokenizers.en = CustomTokenizer(reserved_tokens, './tokenizer/subword/en_vocab.txt')"
      ],
      "metadata": {
        "id": "tyS0Jfl2LK4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = './tokenizer/subword/en_ar_tokenizer'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "metadata": {
        "id": "gq8cgPqGLMeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers = tf.saved_model.load(model_name)\n",
        "tokenizers.en.get_vocab_size().numpy()"
      ],
      "metadata": {
        "id": "MkpW8VBoLNcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizers.en.tokenize(['Hello Mohamed !'])\n",
        "tokens.numpy()"
      ],
      "metadata": {
        "id": "oyAEVUD6LOcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens = tokenizers.en.lookup(tokens)\n",
        "text_tokens"
      ],
      "metadata": {
        "id": "wX7vcWrcLPzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round_trip = tokenizers.en.detokenize(tokens)\n",
        "\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "id": "4MaNoI6BLRC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS=128\n",
        "def prepare_batch(en, ar):\n",
        "    en = tokenizers.en.tokenize(en)      # Output is ragged.\n",
        "    en = en[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
        "    en = en.to_tensor()  # Convert to 0-padded dense Tensor\n",
        "\n",
        "    ar = tokenizers.ar.tokenize(ar)\n",
        "    ar = ar[:, :(MAX_TOKENS+1)]\n",
        "    ar_inputs = ar[:, :-1].to_tensor()  # Drop the [END] tokens\n",
        "    ar_labels = ar[:, 1:].to_tensor()   # Drop the [START] tokens\n",
        "\n",
        "    return (en, ar_inputs), ar_labels"
      ],
      "metadata": {
        "id": "e_fxgfliLSdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  )"
      ],
      "metadata": {
        "id": "5BHdmUp4LTrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(train_ds)\n",
        "val_batches = make_batches(validation_ds)"
      ],
      "metadata": {
        "id": "H-n14bTFLUvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (en_i, ar_i), ar_o in train_batches.take(1):\n",
        "    print(tokenizers.en.detokenize(en_i)[0])\n",
        "    print(tokenizers.ar.detokenize(ar_i)[0].numpy().decode())\n",
        "    print(tokenizers.ar.detokenize(ar_o)[0].numpy().decode())\n",
        "    print(tokenizers.en.lookup(en_i)[0])\n",
        "    print(tokenizers.ar.lookup(ar_i)[0])\n",
        "    print(tokenizers.ar.lookup(ar_o)[0])"
      ],
      "metadata": {
        "id": "ZzwG-IasLVtH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
